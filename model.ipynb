{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11889\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = open('data.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 22, 100)           1188900   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300)               301200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11889)             3578589   \n",
      "=================================================================\n",
      "Total params: 5,068,689\n",
      "Trainable params: 5,068,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150 )))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "print( model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5479/5479 [==============================] - 55s 9ms/step - loss: 6.2362 - accuracy: 0.0852\n",
      "Epoch 2/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 5.4628 - accuracy: 0.1366\n",
      "Epoch 3/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 5.0794 - accuracy: 0.1581\n",
      "Epoch 4/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 4.7687 - accuracy: 0.1753\n",
      "Epoch 5/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 4.4899 - accuracy: 0.1932\n",
      "Epoch 6/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 4.2390 - accuracy: 0.2131\n",
      "Epoch 7/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 4.0118 - accuracy: 0.2361\n",
      "Epoch 8/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 3.8101 - accuracy: 0.2592\n",
      "Epoch 9/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 3.6257 - accuracy: 0.2833\n",
      "Epoch 10/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 3.4604 - accuracy: 0.3073\n",
      "Epoch 11/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 3.3125 - accuracy: 0.3298\n",
      "Epoch 12/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 3.1796 - accuracy: 0.3507\n",
      "Epoch 13/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 3.0607 - accuracy: 0.3702\n",
      "Epoch 14/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 2.9530 - accuracy: 0.3882\n",
      "Epoch 15/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 2.8570 - accuracy: 0.4041\n",
      "Epoch 16/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 2.7713 - accuracy: 0.4187\n",
      "Epoch 17/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 2.6901 - accuracy: 0.4324\n",
      "Epoch 18/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 2.6179 - accuracy: 0.4448\n",
      "Epoch 19/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 2.5526 - accuracy: 0.4576\n",
      "Epoch 20/100\n",
      "5479/5479 [==============================] - 57s 10ms/step - loss: 2.4897 - accuracy: 0.4695\n",
      "Epoch 21/100\n",
      "5479/5479 [==============================] - 56s 10ms/step - loss: 2.4350 - accuracy: 0.4787\n",
      "Epoch 22/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 2.3819 - accuracy: 0.4891\n",
      "Epoch 23/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 2.3347 - accuracy: 0.4978\n",
      "Epoch 24/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 2.2881 - accuracy: 0.5066\n",
      "Epoch 25/100\n",
      "5479/5479 [==============================] - 56s 10ms/step - loss: 2.2464 - accuracy: 0.5136\n",
      "Epoch 26/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 2.2064 - accuracy: 0.5212\n",
      "Epoch 27/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 2.1704 - accuracy: 0.5274\n",
      "Epoch 28/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 2.1353 - accuracy: 0.5342\n",
      "Epoch 29/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 2.1029 - accuracy: 0.5397\n",
      "Epoch 30/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 2.0729 - accuracy: 0.5462\n",
      "Epoch 31/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 2.0454 - accuracy: 0.5517\n",
      "Epoch 32/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 2.0188 - accuracy: 0.5561\n",
      "Epoch 33/100\n",
      "5479/5479 [==============================] - 52s 9ms/step - loss: 1.9964 - accuracy: 0.5599\n",
      "Epoch 34/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.9736 - accuracy: 0.5647\n",
      "Epoch 35/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.9509 - accuracy: 0.5685\n",
      "Epoch 36/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.9314 - accuracy: 0.5729\n",
      "Epoch 37/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.9121 - accuracy: 0.5747\n",
      "Epoch 38/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.8933 - accuracy: 0.5792\n",
      "Epoch 39/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.8779 - accuracy: 0.5817\n",
      "Epoch 40/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.8613 - accuracy: 0.5854\n",
      "Epoch 41/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.8462 - accuracy: 0.5865\n",
      "Epoch 42/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.8310 - accuracy: 0.5897\n",
      "Epoch 43/100\n",
      "5479/5479 [==============================] - 57s 10ms/step - loss: 1.8177 - accuracy: 0.5912\n",
      "Epoch 44/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.8060 - accuracy: 0.5951\n",
      "Epoch 45/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.7940 - accuracy: 0.5965\n",
      "Epoch 46/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.7835 - accuracy: 0.5981\n",
      "Epoch 47/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.7720 - accuracy: 0.6000\n",
      "Epoch 48/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.7622 - accuracy: 0.6016\n",
      "Epoch 49/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.7534 - accuracy: 0.6032\n",
      "Epoch 50/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.7428 - accuracy: 0.6057\n",
      "Epoch 51/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.7350 - accuracy: 0.6059\n",
      "Epoch 52/100\n",
      "5479/5479 [==============================] - 56s 10ms/step - loss: 1.7288 - accuracy: 0.6075\n",
      "Epoch 53/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.7182 - accuracy: 0.6094\n",
      "Epoch 54/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.7109 - accuracy: 0.6097\n",
      "Epoch 55/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.7068 - accuracy: 0.6118\n",
      "Epoch 56/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.6983 - accuracy: 0.6127\n",
      "Epoch 57/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.6962 - accuracy: 0.6116\n",
      "Epoch 58/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.6859 - accuracy: 0.6144\n",
      "Epoch 59/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.6841 - accuracy: 0.6144\n",
      "Epoch 60/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.6758 - accuracy: 0.6166\n",
      "Epoch 61/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.6732 - accuracy: 0.6158\n",
      "Epoch 62/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.6669 - accuracy: 0.6168\n",
      "Epoch 63/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.6654 - accuracy: 0.6175\n",
      "Epoch 64/100\n",
      "5479/5479 [==============================] - 52s 9ms/step - loss: 1.6617 - accuracy: 0.6177\n",
      "Epoch 65/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.6549 - accuracy: 0.6199\n",
      "Epoch 66/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.6526 - accuracy: 0.6188\n",
      "Epoch 67/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.6518 - accuracy: 0.6189\n",
      "Epoch 68/100\n",
      "5479/5479 [==============================] - 54s 10ms/step - loss: 1.6470 - accuracy: 0.6204\n",
      "Epoch 69/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.6444 - accuracy: 0.6199\n",
      "Epoch 70/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.6412 - accuracy: 0.6211\n",
      "Epoch 71/100\n",
      "5479/5479 [==============================] - 55s 10ms/step - loss: 1.6363 - accuracy: 0.6214\n",
      "Epoch 72/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.6306 - accuracy: 0.6219\n",
      "Epoch 73/100\n",
      "5479/5479 [==============================] - 52s 10ms/step - loss: 1.6307 - accuracy: 0.6214\n",
      "Epoch 74/100\n",
      "5479/5479 [==============================] - 53s 10ms/step - loss: 1.6292 - accuracy: 0.6216\n",
      "Epoch 75/100\n",
      "3615/5479 [==================>...........] - ETA: 18s - loss: 1.5754 - accuracy: 0.6334"
     ]
    }
   ],
   "source": [
    "model.fit(xs,ys , epochs = 100 , callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = open('data.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "seed_text = \"Iron Man  : \"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
